{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an RDD from a list of numbers (1,50) using numpy methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nums=np.arange(1,51)\n",
        "rdd1=sc.parallelize(nums)\n",
        "rdd1.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the sum, average, maximum, minimum, and count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1275\n"
          ]
        }
      ],
      "source": [
        "#Sum\n",
        "sum=rdd1.sum()\n",
        "print(sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25.5\n"
          ]
        }
      ],
      "source": [
        "#avg\n",
        "avg=sum/rdd1.count()\n",
        "print(avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Count\n",
        "rdd1.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Min\n",
        "rdd1.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Max\n",
        "rdd1.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count how many numbers are even vs. odd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "even count is  25\n",
            "odd ount is  25\n"
          ]
        }
      ],
      "source": [
        "even_count=rdd1.filter(lambda X: X % 2 ==0).count()\n",
        "\n",
        "odd_count =rdd1.filter( lambda x : x % 2 != 0).count()\n",
        "\n",
        "print('even count is ',even_count)\n",
        "print('odd ount is ',odd_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have the following data of people info ('Name', 'Age'), answer the following questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Nada ', 25),\n",
              " ('Mona', 30),\n",
              " ('Ahmed', 35),\n",
              " ('Khaled', 40),\n",
              " ('Ahmed', 35),\n",
              " ('Nada ', 25)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "people_data = [(\"Nada \", 25), (\"Mona\", 30), (\"Ahmed\", 35), (\"Khaled\", 40),(\"Ahmed\", 35), ('Nada ', 25)]\n",
        "rdd_people = sc.parallelize(people_data)\n",
        "rdd_people.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the oldest person"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "oldset is ('Khaled', 40)\n"
          ]
        }
      ],
      "source": [
        "oldest_person= rdd_people.max(key=lambda x:x [1])\n",
        "\n",
        "print('oldset is',oldest_person)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the average age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg age : 31.666666666666668\n"
          ]
        }
      ],
      "source": [
        "ages_rdd=rdd_people.map(lambda x:x[1])\n",
        "total_age=ages_rdd.sum()\n",
        "count_people=ages_rdd.count()\n",
        "avg_age = total_age / count_people \n",
        "print('avg age :',avg_age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group all the names by their age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(25, [('Nada ', 25), ('Nada ', 25)]), (30, [('Mona', 30)]), (35, [('Ahmed', 35), ('Ahmed', 35)]), (40, [('Khaled', 40)])]\n"
          ]
        }
      ],
      "source": [
        "rdd_by_age = rdd_people.map(lambda x: (x[1], x))\n",
        "grouped = rdd_by_age.groupByKey().mapValues(list)\n",
        "\n",
        "\n",
        "result = grouped.sortByKey()\n",
        "\n",
        "print(result.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take the following text and put it in a text file named russia.txt and load it into rdd\n",
        "\n",
        "\"Russia is the largest country in the world by land area\n",
        "Moscow is the capital city of Russia\n",
        "The Russian language is one of the most widely spoken languages in the world\n",
        "Russia is known for its rich history and culture\n",
        "The Trans-Siberian Railway is the longest railway line in the world\n",
        "Russia has a strong tradition in literature, music and ballet\n",
        "The country is famous for its cold winters and vast landscapes\n",
        "Russia is a major player in global energy production\n",
        "\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4ebe0ba78707:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x775f318b6090>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"['Russia is the largest country in the world by land area',\",\n",
              " \" 'Moscow is the capital city of Russia',\",\n",
              " \" 'The Russian language is one of the most widely spoken languages in the world',\",\n",
              " \" 'Russia is known for its rich history and culture',\",\n",
              " \" 'The Trans-Siberian Railway is the longest railway line in the world',\",\n",
              " \" 'Russia has a strong tradition in literature, music and ballet',\",\n",
              " \" 'The country is famous for its cold winters and vast landscapes',\",\n",
              " \" 'Russia is a major player in global energy production']\"]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_russia = sc.textFile(\"/data/russia.txt\")\n",
        "rdd_russia.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count the total number of lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no of line: 8\n"
          ]
        }
      ],
      "source": [
        "line_count=rdd_russia.count()\n",
        "print('no of line:',line_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count how many lines contain the word \"Russia\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no : 6\n"
          ]
        }
      ],
      "source": [
        "russia_line=rdd_russia.filter(lambda line: \"Russia\" in line ).count()\n",
        "print('no :',russia_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the most 5 frequent word in the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top5: [('is', 7), ('the', 7), ('in', 5), (\"'The\", 3), ('and', 3)]\n"
          ]
        }
      ],
      "source": [
        "words_rdd=rdd_russia.flatMap(lambda line: line.split())\n",
        "word_ones=words_rdd.map(lambda word : (word,1))\n",
        "word_counts=word_ones.reduceByKey(lambda a,b : a + b)\n",
        "\n",
        "sorted_words=word_counts.sortBy(lambda x: x [1],ascending=False)\n",
        "TOP5=sorted_words.take(5)\n",
        "print('top5:',TOP5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenize words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['russia', 'is', 'the', 'largest', 'country', 'in', 'the', 'world', 'by', 'land', 'area', 'moscow', 'is', 'the', 'capital', 'city', 'of', 'russia', 'the', 'russian']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "tokens_rdd = rdd_russia.flatMap(\n",
        "    lambda line: re.findall(r\"\\b\\w+\\b\", line.lower())\n",
        ")\n",
        "\n",
        "\n",
        "print(tokens_rdd.take(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove stopwords (a, the, is, to, in, of). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['russia', 'largest', 'country', 'world', 'by', 'land', 'area', 'moscow', 'capital', 'city', 'russia', 'russian', 'language', 'one', 'most', 'widely', 'spoken', 'languages', 'world', 'russia']\n"
          ]
        }
      ],
      "source": [
        "stopwords={'a','the','is','to','in','of'}\n",
        "\n",
        "filter_stop=tokens_rdd.filter(lambda word : word not in stopwords)\n",
        "\n",
        "print(filter_stop.take(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count the frequency of each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\"['Russia\", 1), ('largest', 1), ('country', 2), ('world', 1), ('by', 1), ('land', 1), (\"area',\", 1), ('capital', 1), ('of', 2), (\"Russia',\", 1), (\"'The\", 3), ('language', 1), ('most', 1), ('widely', 1), (\"world',\", 2), ('known', 1), ('for', 2), ('history', 1), ('and', 3), (\"culture',\", 1)]\n"
          ]
        }
      ],
      "source": [
        "print(word_counts.take(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema = 'id integer, name string, age integer, salary integer' \n",
        "data = [\n",
        "    (1, \"Ali\", 25, 4000),\n",
        "    (2, \"Mariam\", 30, 6000),\n",
        "    (3, \"Omar\", 35, 7000),\n",
        "    (4, \"Sara\", 28, 5000),\n",
        "    (5, \"Omar\", 25, 6500),\n",
        "    (6, \"Mariam\", 26, 7500)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data,schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show schema and first 2 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---+------+---+------+\n",
            "| id|  name|age|salary|\n",
            "+---+------+---+------+\n",
            "|  1|   Ali| 25|  4000|\n",
            "|  2|Mariam| 30|  6000|\n",
            "+---+------+---+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()\n",
        "\n",
        "\n",
        "df.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select only name and salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "|  name|salary|\n",
            "+------+------+\n",
            "|   Ali|  4000|\n",
            "|Mariam|  6000|\n",
            "|  Omar|  7000|\n",
            "|  Sara|  5000|\n",
            "|  Omar|  6500|\n",
            "|Mariam|  7500|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"name\",\"salary\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the average salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|avg_salary|\n",
            "+----------+\n",
            "|    6000.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df.select(F.avg('salary').alias(\"avg_salary\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Filter employees older than 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+---+------+\n",
            "| id|  name|age|salary|\n",
            "+---+------+---+------+\n",
            "|  2|Mariam| 30|  6000|\n",
            "|  3|  Omar| 35|  7000|\n",
            "+---+------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.age>28).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Count distinct values in the name column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|distinct_names|\n",
            "+--------------+\n",
            "|             4|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(F.countDistinct(\"name\").alias(\"distinct_names\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Group by a the name column and find average salary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+\n",
            "|  name|avg_salary|\n",
            "+------+----------+\n",
            "|   Ali|    4000.0|\n",
            "|Mariam|    6750.0|\n",
            "|  Omar|    6750.0|\n",
            "|  Sara|    5000.0|\n",
            "+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.groupBy(\"name\").agg(F.avg(\"salary\").alias(\"avg_salary\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|PK\u0003\u0004\u0014\u0000\\b\\b\\b\u0000�P9[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/drawings/drawing1.xml��]n�0\\f\\a�\u0013�\u000eU�iZ\u0018\u0013C\u0014^�N0\u000e�%n\u001b���\u000e��~�J6i{\u0001\u001em�?���nt��Db\u0013|#�\u0012\u0005z\u0015��]#\u000e�o��(8��`��F\\��n��\u00195�ϼ�\"�{^��\u0011}��ZJV=:�2\\f�Ӵ|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                       � ��:�\\t�IvVΫ�E�@...|\n",
            "|                                                                                                                                                       �A�\u001c�8!�\u0016b�\u0005�f଩\\a...|\n",
            "|                                                                                                                                                       0$\\aL�BۛB\\f/iĐ�w�...|\n",
            "|                                                                                                                                                       Xd����\u001fJ\u0006u���1�\u0016b...|\n",
            "|                                                                                                                                                       �0\u0010\u0006�\u0013x�0{�օ�4�F�...|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = spark.read.csv(\"/data/NullData.csv\", header=True, inferSchema=True) #this file in shared folder\n",
        "df1.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the avg sales "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Sales` cannot be resolved. Did you mean one of the following? [`PK\u0003\u0004\u0014\u0000\b\b\b\u0000�P9[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/drawings/drawing1.xml��]n�0\f\u0007�\u0013�\u000eU�iZ\u0018\u0013C\u0014^�N0\u000e�%n\u001b���\u000e��~�J6i{\u0001\u001em�?���nt��Db\u0013|#�\u0012\u0005z\u0015��]#\u000e�o��(8��``��F\\��n��\u00195�ϼ�\"�{^��\u0011}��ZJV=:�2\f�Ӵ`].;\n'Aggregate [avg('Sales) AS avg_sale#240]\n+- Relation [PK\u0003\u0004\u0014\u0000\b\b\b\u0000�P9[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/drawings/drawing1.xml��]n�0\f\u0007�\u0013�\u000eU�iZ\u0018\u0013C\u0014^�N0\u000e�%n\u001b���\u000e��~�J6i{\u0001\u001em�?���nt��Db\u0013|#�\u0012\u0005z\u0015��]#\u000e�o��(8��`��F\\��n��\u00195�ϼ�\"�{^��\u0011}��ZJV=:�2\f�Ӵ#230] csv\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_sale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
            "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Sales` cannot be resolved. Did you mean one of the following? [`PK\u0003\u0004\u0014\u0000\b\b\b\u0000�P9[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/drawings/drawing1.xml��]n�0\f\u0007�\u0013�\u000eU�iZ\u0018\u0013C\u0014^�N0\u000e�%n\u001b���\u000e��~�J6i{\u0001\u001em�?���nt��Db\u0013|#�\u0012\u0005z\u0015��]#\u000e�o��(8��``��F\\��n��\u00195�ϼ�\"�{^��\u0011}��ZJV=:�2\f�Ӵ`].;\n'Aggregate [avg('Sales) AS avg_sale#240]\n+- Relation [PK\u0003\u0004\u0014\u0000\b\b\b\u0000�P9[\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0018\u0000\u0000\u0000xl/drawings/drawing1.xml��]n�0\f\u0007�\u0013�\u000eU�iZ\u0018\u0013C\u0014^�N0\u000e�%n\u001b���\u000e��~�J6i{\u0001\u001em�?���nt��Db\u0013|#�\u0012\u0005z\u0015��]#\u000e�o��(8��`��F\\��n��\u00195�ϼ�\"�{^��\u0011}��ZJV=:�2\f�Ӵ#230] csv\n"
          ]
        }
      ],
      "source": [
        "df1.select(F.avg(\"Sales\").alias(\"avg_sale\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace null name with 'Unknown' and sales with the avg sales of the column "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+-------+-----+\n",
            "|  Id|   Name|Sales|\n",
            "+----+-------+-----+\n",
            "|emp1|   John|400.5|\n",
            "|emp2|Unknown|400.5|\n",
            "|emp3|Unknown|345.0|\n",
            "|emp4|  Cindy|456.0|\n",
            "+----+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
