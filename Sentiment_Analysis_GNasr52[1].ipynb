{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Sr46iwiMAZ"
      },
      "source": [
        "## Sentiment Analysis using RNN\n",
        "\n",
        "Text Classification Task: Sentiment Analysis\n",
        "\n",
        "- Data Preparation: Today we use a dataset (IMDb) for movie reviews. Each review is labeled as either positive or negative.\n",
        "\n",
        "- Preprocessing: Tokenize the text and convert words to integers. Pad sequences to ensure they have the same length.\n",
        "\n",
        "- Model Definition:\n",
        "using an RNN layer to capture the sequential nature of the reviews.\n",
        "Add a Dense layer with a sigmoid activation for binary classification.\n",
        "\n",
        "- Training:\n",
        "Train the model on the training dataset.\n",
        "\n",
        "- Evaluation:\n",
        "Test on a separate validation set and evaluate performance using metrics like accuracy or F1-score.\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "In this, we have to predict the number of positive and negative reviews based on sentiments by using RNN archticture. This is workable example on Many to One type as it takes sentances and output if it's negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnLdD0EYpVYL",
        "outputId": "1761d665-b3dd-48c5-b62f-c1c0d0599393"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\yostina\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yostina\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\yostina\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Embedding\n",
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import unicodedata\n",
        "import html\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D704hDJqucrM",
        "outputId": "332c6b0e-339f-4de8-c125-50ce794598ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: kaggle in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (4.67.0)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: colorama in c:\\users\\yostina\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->kaggle) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eNwHfQd0Kd0",
        "outputId": "34c5ee24-b2d0-4ee9-ed81-0e9ecd6d727d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUh9hBb40V_U",
        "outputId": "c6fe126d-3444-4fc9-da88-23e950f13777"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7RGI9_6ySnP",
        "outputId": "c3304191-10bc-4bc3-9658-c6bbed283859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ],
      "source": [
        "raw_data = pd.read_csv(r\"C:\\Users\\yostina\\Desktop\\archive (2).zip\")\n",
        "print(raw_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR5el2yL9adg",
        "outputId": "7b5158f1-436d-4337-ca42-53f62edefdfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 40000\n",
            "Testing set size: 10000\n"
          ]
        }
      ],
      "source": [
        "X = raw_data['review']  # Features: reviews\n",
        "raw_data['label'] = raw_data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "y = raw_data['label']  # Labels: sentiment (positive/negative)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "922DHrktiSUM"
      },
      "source": [
        "### Data Prerocessing Pipeline\n",
        "\n",
        "- remove_special_chars(text)\n",
        "\n",
        "Purpose: Clean the input text by removing special characters and HTML entities.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compile a regex to match multiple spaces.\n",
        "\n",
        "Convert text to lowercase.\n",
        "\n",
        "Replace specific HTML character codes with their corresponding characters.\n",
        "\n",
        "Replace newline characters and HTML tags with appropriate representations.\n",
        "\n",
        "Use html.unescape to convert any remaining HTML entities.\n",
        "\n",
        "Replace multiple spaces with a single space.\n",
        "\n",
        "- remove_non_ascii(text)\n",
        "\n",
        "Purpose: Eliminate non-ASCII characters from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Normalize the text to a compatible Unicode format.\n",
        "\n",
        "Encode to ASCII, ignoring non-ASCII characters.\n",
        "\n",
        "Decode back to UTF-8 format.\n",
        "\n",
        "- to_lowercase(text)\n",
        "\n",
        "Purpose: Convert all characters in the text to lowercase.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Simply return the text converted to lowercase.\n",
        "\n",
        "- remove_punctuation(text)\n",
        "\n",
        "Purpose: Strip punctuation from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create a translation table that maps punctuation characters to None.\n",
        "\n",
        "Use the translation table to translate the text.\n",
        "\n",
        "- replace_numbers(text)\n",
        "\n",
        "Purpose: Remove all integer occurrences from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Use a regex to find and replace all digits with an empty string.\n",
        "\n",
        "- remove_whitespaces(text)\n",
        "\n",
        "Purpose: Trim leading and trailing whitespace from the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Return the text after applying the strip() method.\n",
        "\n",
        "- remove_stopwords(words, stop_words)\n",
        "\n",
        "Purpose: Filter out common stopwords from a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Return a list of words that are not present in the provided stop_words set.\n",
        "\n",
        "- stem_words(words)\n",
        "\n",
        "Purpose: Apply stemming to a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a stemmer.\n",
        "\n",
        "Return a list of stemmed words using the stemmer.\n",
        "\n",
        "- lemmatize_words(words)\n",
        "\n",
        "Purpose: Lemmatize words in the text to their base form.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a lemmatizer.\n",
        "\n",
        "Return a list of lemmatized words.\n",
        "\n",
        "- lemmatize_verbs(words)\n",
        "\n",
        "Purpose: Specifically lemmatize verbs in the text.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Create an instance of a lemmatizer.\n",
        "\n",
        "Return a string of lemmatized verbs, maintaining space between words.\n",
        "\n",
        "- text2words(text)\n",
        "\n",
        "Purpose: Tokenize the input text into a list of words.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Use a word tokenizer to split the text into individual words and return the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8jRsBGaZ8MFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_special_chars(text):\n",
        "    re1 = re.compile(r'  +')\n",
        "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
        "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x1))\n",
        "\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "def replace_numbers(text):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "\n",
        "def remove_whitespaces(text):\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stop_words):\n",
        "    \"\"\"\n",
        "    :param words:\n",
        "    :type words:\n",
        "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "    or\n",
        "    from spacy.lang.en.stop_words import STOP_WORDS\n",
        "    :type stop_words:\n",
        "    :return:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in text\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in words]\n",
        "\n",
        "def lemmatize_words(words):\n",
        "    \"\"\"Lemmatize words in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in text\"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
        "\n",
        "def text2words(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "def normalize_text( text):\n",
        "    text = remove_special_chars(text)\n",
        "    text = remove_non_ascii(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = replace_numbers(text)\n",
        "    words = text2words(text)\n",
        "    words = remove_stopwords(words, stop_words)\n",
        "    # words = stem_words(words)# Either stem ovocar lemmatize\n",
        "    words = lemmatize_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "\n",
        "    return ''.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HZzQakmp8U4P"
      },
      "outputs": [],
      "source": [
        "def normalize_corpus(corpus):\n",
        "  return [normalize_text(t) for t in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "myIUvjmm8bIh"
      },
      "outputs": [],
      "source": [
        "proc_X_train = normalize_corpus(X_train)\n",
        "proc_X_test = normalize_corpus(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH1-otO3_4ur",
        "outputId": "473528e6-71c0-4c57-fb3b-379785c19c8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH3RF9EANJTr",
        "outputId": "433320ce-0e60-43e5-95a7-51de46dc3b5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVj7vxZziYpm"
      },
      "source": [
        "### Building Pre-trained word embeddings using Glove6B With Bi-Directional LSTM\n",
        "\n",
        "Overview of Layers\n",
        "\n",
        "- Embedding Layer:\n",
        "\n",
        "This layer initializes with the pre-trained GloVe embeddings.\n",
        "\n",
        "You will need to load the GloVe vectors and create an embedding matrix where each word index corresponds to its GloVe vector.\n",
        "\n",
        "- Bidirectional LSTM Layer:\n",
        "\n",
        "This layer processes sequences in both forward and backward directions, capturing context from both sides.\n",
        "It consists of two LSTM layers: one for the forward pass and another for the backward pass.\n",
        "\n",
        "- Dense Layer(s):\n",
        "\n",
        "Typically, you'll have one or more fully connected layers to output your final predictions.\n",
        "The last dense layer often uses a softmax activation for classification tasks.\n",
        "\n",
        "- Output Layer:\n",
        "\n",
        "This layer generates the final predictions, which can be class labels, probabilities, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تم فك الضغط بنجاح!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = r\"C:\\Users\\yostina\\Desktop\\glove.6B.zip\"  # المسار إلى ملف ZIP\n",
        "extract_to = r\"C:\\Users\\yostina\\Desktop\\glove\"       # المجلد الذي سيتم فك الضغط إليه\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"تم فك الضغط بنجاح!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "0Olj82_w4rE_",
        "outputId": "4730068c-c796-4ca8-f1be-80b49a2fa411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yostina\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,297,500</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │    \u001b[38;5;34m12,297,500\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,297,500</span> (46.91 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,297,500\u001b[0m (46.91 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,297,500</span> (46.91 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,297,500\u001b[0m (46.91 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 70ms/step - acc: 0.7037 - loss: 0.5498\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - acc: 0.8435 - loss: 0.3599\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 67ms/step - acc: 0.8679 - loss: 0.3146\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 59ms/step - acc: 0.8798 - loss: 0.2923\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 57ms/step - acc: 0.8897 - loss: 0.2707\n",
            "Test Accuracy: 87.040001\n"
          ]
        }
      ],
      "source": [
        "# Prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(proc_X_train)  # Fit on training data only\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# Integer encode the training documents\n",
        "encoded_train_docs = t.texts_to_sequences(proc_X_train)\n",
        "# Integer encode the testing documents\n",
        "encoded_test_docs = t.texts_to_sequences(proc_X_test)\n",
        "\n",
        "# Pad documents to a max length of 100 words (adjust as necessary)\n",
        "max_length = 100\n",
        "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the whole embedding into memory (make sure to have the GloVe file)\n",
        "embeddings_index = dict()\n",
        "with open(r\"C:\\Users\\yostina\\Desktop\\glove\\glove.6B.100d.txt\", mode='rt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define model using GRU\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
        "model.add(GRU(100))  # GRU layer\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(padded_train_docs, y_train, epochs=5, batch_size = 32, verbose=1)  # Reduced epochs for quicker training\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(padded_test_docs, y_test, verbose=0)\n",
        "print('Test Accuracy: %f' % (accuracy * 100))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
